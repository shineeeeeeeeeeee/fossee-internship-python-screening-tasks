Research Plan: The Open Source Models of Student Competence Analysis.

Research Plan (2 Paragraphs)

My methodology to assess open-source models to analyze high-level
student competence starts with finding AI models that have been designed
to understand code and natural language processing. Given that the test
case is Python learning, I will pay attention to freely available models
like CodeBERT or CodeT5 which are models trained on extensive
programming languages corpora. These models are able to analyze Python
code written by students and find semantic meaning in addition to syntax
correctness. I will also take into account the lightweight NLP tools
such as NLTK or spaCy to create the prompts that prompt further learning
without giving the direct answers. Through the assessment, it will be
determined whether or not the model can point out logical
inconsistencies, misconceptions, or omissions in student code.

The assessment criteria will encompass accuracy (capability to detect
the presence of misconceptions correctly), interpretability (of whether
the model can give feedback in a way that is understandable by students
and teachers), cost and accessibility (easiness to run in a local
environment with no heavy requirements in terms of using a powerful
GPU), and scalability (ability to scale to classroom-sized tasks). When
the applicability has to be checked, I will test the model using a
sample student Python code with purposeful errors (e.g., incorrect loop
conditions, wrong usage of recursion, or wrong data structures). I will
then examine whether the model is capable of providing prompts that
judge the conceptual gaps on the part of the student as opposed to
presenting the answer.

Reasoning

What is suitable with a model of high level competence analysis?\
An appropriate model should not just be limited to syntax correction but
examine depth of concepts. It must spot logical gaps, misconceptions,
and fallacies and should give feedback whereby it does not spoon feed
answers to the person, but gives feedback that motivates them to
reflect.

How can you test the meaningfulness of prompts being generated by a
model?\
To test what is happening, I would enter Python code examples with
common syntax errors and would verify that the model would produce
prompts such as, "Why does this loop never terminate? or What is the
difference between shallowness and deepness copy here? These cues ought
to stimulate, not solve, but instead get people to have deeper insight
into the code.

Are there any trade-offs between accuracy, interpretability and cost?\
Precision: Bigger models of transformers (such as CodeT5) are more
precise but demand more resources.\
Interpretability: Less complex NLP models (such as NLTK) are simpler to
interpret, but can overlook minor errors.\
Price: It might not be possible to run heavy models on a classroom that
does not have GPUs. Therefore, there should be a balance that will be
struck.

What was the reason behind your selection of the model you have
considered, and what are the strengths or weaknesses of the model?\
The main candidate that I selected is CodeT5 since it is open-source,
pre-trained on various programming languages, and specifically meant to
be used on code understanding and code generation. Its advantages are a
contextual knowledge of the Python syntax and semantics. Its weaknesses,
however, are increased computational expense and fine-tuning to apply it
to educational use.
